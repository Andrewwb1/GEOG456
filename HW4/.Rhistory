mean(knn.pred == Direction.2009)
train.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[train , ] #A matrix containing the predictors associated with the training data
test.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[!train , ] #A matrix containing the predictors associated with the data for which we wish to make predictions
train.Direction <- Weekly$Direction[train] #A vector containing the class labels for the training observations
knn.pred <- knn(train.X, test.X, train.Direction , k = 5) #train and test in one line
mean(knn.pred == Direction.2009)
train.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[train , ] #A matrix containing the predictors associated with the training data
test.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[!train , ] #A matrix containing the predictors associated with the data for which we wish to make predictions
train.Direction <- Weekly$Direction[train] #A vector containing the class labels for the training observations
knn.pred <- knn(train.X, test.X, train.Direction , k = 7) #train and test in one line
mean(knn.pred == Direction.2009)
train.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[train , ] #A matrix containing the predictors associated with the training data
test.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[!train , ] #A matrix containing the predictors associated with the data for which we wish to make predictions
train.Direction <- Weekly$Direction[train] #A vector containing the class labels for the training observations
knn.pred <- knn(train.X, test.X, train.Direction , k = 12) #train and test in one line
mean(knn.pred == Direction.2009)
train.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[train , ] #A matrix containing the predictors associated with the training data
test.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[!train , ] #A matrix containing the predictors associated with the data for which we wish to make predictions
train.Direction <- Weekly$Direction[train] #A vector containing the class labels for the training observations
knn.pred <- knn(train.X, test.X, train.Direction , k = 7) #train and test in one line
mean(knn.pred == Direction.2009)
train.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[train , ] #A matrix containing the predictors associated with the training data
test.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[!train , ] #A matrix containing the predictors associated with the data for which we wish to make predictions
train.Direction <- Weekly$Direction[train] #A vector containing the class labels for the training observations
knn.pred <- knn(train.X, test.X, train.Direction , k = 8) #train and test in one line
mean(knn.pred == Direction.2009)
train.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[train , ] #A matrix containing the predictors associated with the training data
test.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[!train , ] #A matrix containing the predictors associated with the data for which we wish to make predictions
train.Direction <- Weekly$Direction[train] #A vector containing the class labels for the training observations
knn.pred <- knn(train.X, test.X, train.Direction , k = 9) #train and test in one line
mean(knn.pred == Direction.2009)
train.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[train , ] #A matrix containing the predictors associated with the training data
test.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[!train , ] #A matrix containing the predictors associated with the data for which we wish to make predictions
train.Direction <- Weekly$Direction[train] #A vector containing the class labels for the training observations
knn.pred <- knn(train.X, test.X, train.Direction , k = 7) #train and test in one line
mean(knn.pred == Direction.2009)
train.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[train , ] #A matrix containing the predictors associated with the training data
test.X <- cbind(Weekly$Lag1 , Weekly$Lag2, Weekly$Lag3)[!train , ] #A matrix containing the predictors associated with the data for which we wish to make predictions
train.Direction <- Weekly$Direction[train] #A vector containing the class labels for the training observations
knn.pred <- knn(train.X, test.X, train.Direction , k = 8) #train and test in one line
mean(knn.pred == Direction.2009)
## Naive Bayes FUN ##
nb.fit <- naiveBayes(Direction ~ . , data = Weekly ,subset = train)
nb.class <- predict(nb.fit , Weekly.2009)
table(nb.class , Direction.2009)
mean(nb.class == Direction.2009)
## Naive Bayes FUN ##
nb.fit <- naiveBayes(Direction ~ Lag1*Lag2 , data = Weekly ,subset = train)
nb.class <- predict(nb.fit , Weekly.2009)
table(nb.class , Direction.2009)
## Naive Bayes FUN ##
nb.fit <- naiveBayes(Direction ~ Lag1 + Lag2 , data = Weekly ,subset = train)
nb.class <- predict(nb.fit , Weekly.2009)
table(nb.class , Direction.2009)
mean(nb.class == Direction.2009)
## Naive Bayes FUN ##
nb.fit <- naiveBayes(Direction ~ Lag1 + Lag2 + Lag3 , data = Weekly ,subset = train)
nb.class <- predict(nb.fit , Weekly.2009)
table(nb.class , Direction.2009)
mean(nb.class == Direction.2009)
install.packages(c("bit", "bit64", "bslib", "cards", "class", "classInt", "cli", "cluster", "collapse", "cpp11", "curl", "data.table", "doBy", "evaluate", "foreign", "gtsummary", "jqr", "jsonlite", "jsonvalidate", "KernSmooth", "later", "lubridate", "nlme", "nnet", "openssl", "pillar", "plm", "processx", "promises", "protolite", "ps", "purrr", "R6", "raster", "Rcpp", "renv", "rlang", "rpart", "s2", "sf", "shiny", "sp", "spatial", "spData", "spdep", "stars", "survival", "systemfonts", "terra", "textshaping", "tinytex", "tmap", "tmaptools", "V8", "xfun", "XML"))
library(gamlr) # loads Matrix as well
help(hockey) # describes the hockey data and shows an example regression
data(hockey) # load the data
##### Processing Detail:
## build matrices of team, situation, and player effects
#
# Create home and away team indicator matrices
# first, make sure the factorization levels match up
goal$team.home <- factor(goal$team.home)
goal$team.away <- factor(goal$team.away)
all(levels(goal$team.home)==levels(goal$team.away))
teams <- levels(goal$team.home) #our list of teams
# An aside... something I get asked often:
#   how do I get indicators for all levels of a factor?
#   i.e., can we avoid dropping the reference level?
# Easiest thing to do is create an extra factor level as reference
goal$team.home <- factor(goal$team.home, levels=c(NA,teams), exclude=NULL)
goal$team.away <- factor(goal$team.away, levels=c(NA,teams), exclude=NULL)
# do a similar thing for goal$season
goal$season <- as.factor(goal$season) # first convert from numeric, then:
goal$season <- factor(goal$season, levels=c(NA,levels(goal$season)), exclude=NULL)
## The exclude=NULL argument is necessary so that R doesn't skip the NA.
## Now the factors have reference level NA: R's symbol for missing data.
levels(goal$team.home)
levels(goal$season)
## get a separate effect for each team in each season
# home team indicators
homemat <- sparse.model.matrix(~ team.home*season, data=goal)[,-1]
# away team version
awaymat <- sparse.model.matrix(~ team.away*season, data=goal)[,-1]
# column names
colnames(homemat)
colnames(awaymat)
# combine them: +1 for home, -1 for away
xteam <- suppressWarnings(homemat-awaymat) # warns about colnames not matching (is OK)
# because I'm obsessive about sensical names
colnames(xteam) <- sub("team.home","",colnames(xteam)) # drop `team.home' from varnames
xteam[1,] # goal 1 is in a game of DAL @ EDM
# also, config contains play configuration info
# e.g., S5v4 is 5 on 4 hockey, +1 if it is for home-team and -1 for away team
config[1,]
##### Analysis
# Combine the covariates all together
x <- cBind(config,xteam,onice) # cBind binds together two sparse matrices
# Combine the covariates all together
x <- cbind(config,xteam,onice) # cBind binds together two sparse matrices
library(gamlr) # loads Matrix as well
help(hockey) # describes the hockey data and shows an example regression
data(hockey) # load the data
##### Processing Detail:
## build matrices of team, situation, and player effects
#
# Create home and away team indicator matrices
# first, make sure the factorization levels match up
goal$team.home <- factor(goal$team.home)
goal$team.away <- factor(goal$team.away)
all(levels(goal$team.home)==levels(goal$team.away))
teams <- levels(goal$team.home) #our list of teams
# An aside... something I get asked often:
#   how do I get indicators for all levels of a factor?
#   i.e., can we avoid dropping the reference level?
# Easiest thing to do is create an extra factor level as reference
goal$team.home <- factor(goal$team.home, levels=c(NA,teams), exclude=NULL)
goal$team.away <- factor(goal$team.away, levels=c(NA,teams), exclude=NULL)
# do a similar thing for goal$season
goal$season <- as.factor(goal$season) # first convert from numeric, then:
goal$season <- factor(goal$season, levels=c(NA,levels(goal$season)), exclude=NULL)
## The exclude=NULL argument is necessary so that R doesn't skip the NA.
## Now the factors have reference level NA: R's symbol for missing data.
levels(goal$team.home)
levels(goal$season)
## get a separate effect for each team in each season
# home team indicators
homemat <- sparse.model.matrix(~ team.home*season, data=goal)[,-1]
# away team version
awaymat <- sparse.model.matrix(~ team.away*season, data=goal)[,-1]
# column names
colnames(homemat)
colnames(awaymat)
# combine them: +1 for home, -1 for away
xteam <- suppressWarnings(homemat-awaymat) # warns about colnames not matching (is OK)
# because I'm obsessive about sensical names
colnames(xteam) <- sub("team.home","",colnames(xteam)) # drop `team.home' from varnames
xteam[1,] # goal 1 is in a game of DAL @ EDM
# also, config contains play configuration info
# e.g., S5v4 is 5 on 4 hockey, +1 if it is for home-team and -1 for away team
config[1,]
##### Analysis
# Combine the covariates all together
x <- cbind(config,xteam,onice) # cBind binds together two sparse matrices
hockey
data(hockey)
summary(hockey)
library(gamlr) # loads Matrix as well
summary(hockey)
rm(list = ls())  # Clear the environment
knitr::opts_knit$set(root.dir = "~/OneDrive - University of North Carolina at Chapel Hill/Spring_25/ECON 573")
library(leaps)
library(ISLR2)
library(glmnet)
library(pls)
#(a)
# set seed
set.seed(1)
# define parameters
n <- 1000
p <- 20
# predictor matrix X
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
# coefficients
betas <- rnorm(p)
betas[14:19] = 0
eps <- rnorm(n, mean = 0, sd = 6) #error
Y <- X %*% betas + eps
# Convert to data frame
rand_data <- data.frame(Y = as.vector(Y), X)
#(b)
#split subset randomly
train_index <- sample(1:n, 100,replace=FALSE)
train <- rand_data[train_index, ]
test <- rand_data[-train_index, ]
#(c)
#best subset selection on training set
train.sub <- regsubsets(Y ~ ., data= train, nvmax = 20)
train.mat <- model.matrix(Y~., data = train)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(train.sub , id = i)
pred <- train.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((train$Y - pred)^2)}
#plot MSE for all sizes
plot(val.errors, main = "Training MSE", type = "l")
# (d)
#best subset selection on test set
test.mat <- model.matrix(Y~., data = test)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(train.sub , id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((test$Y - pred)^2)}
#plot MSE for all sizes
plot(val.errors, main = "Test MSE", type = "l")
# (e)
which.min(val.errors) #where the miniumum MSE occurs
#The smallest test MSE occurs at model size 16. We would expect it to not be the largest model, as there should be a penalty for overfitting.
# (f)
coef(train.sub, id = 16) #coefficients at model size 16
#Model performs pretty poorly given that the true zero values where variables 14-19
# (g)
# NOT DONE YET
#(a)
# set seed
set.seed(1)
# define parameters
n <- 1000
p <- 20
# predictor matrix X
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
# coefficients
betas <- rnorm(p)
betas[14:19] = 0
eps <- rnorm(p, mean = 0, sd = 6) #error
Y <- X %*% betas + eps
# Convert to data frame
rand_data <- data.frame(Y = as.vector(Y), X)
#(b)
#split subset randomly
train_index <- sample(1:n, 100,replace=FALSE)
train <- rand_data[train_index, ]
test <- rand_data[-train_index, ]
#(c)
#best subset selection on training set
train.sub <- regsubsets(Y ~ ., data= train, nvmax = 20)
train.mat <- model.matrix(Y~., data = train)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(train.sub , id = i)
pred <- train.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((train$Y - pred)^2)}
#plot MSE for all sizes
plot(val.errors, main = "Training MSE", type = "l")
# (d)
#best subset selection on test set
test.mat <- model.matrix(Y~., data = test)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(train.sub , id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((test$Y - pred)^2)}
#plot MSE for all sizes
plot(val.errors, main = "Test MSE", type = "l")
# (e)
which.min(val.errors) #where the miniumum MSE occurs
#The smallest test MSE occurs at model size 16. We would expect it to not be the largest model, as there should be a penalty for overfitting.
# (f)
coef(train.sub, id = 16) #coefficients at model size 16
#Model performs pretty poorly given that the true zero values where variables 14-19
# (g)
# NOT DONE YET
View(train)
View(train)
#(a)
# set seed so random draws always same
set.seed(1)
# define parameters
n <- 1000
p <- 20
# predictor matrix X
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
# coefficients
betas <- rnorm(p)
betas[14:19] = 0
eps <- rnorm(p, mean = 0, sd = 6) #error
Y <- X %*% betas + eps
# Convert to data frame
rand_data <- data.frame(Y = as.vector(Y), X)
#(b)
#split subset randomly
train_index <- sample(1:n, 100,replace=FALSE) #random index for sample
train <- rand_data[train_index, ] #training subset
test <- rand_data[-train_index, ] #test subset
#(c)
#best subset selection on training set
train.sub <- regsubsets(Y ~ ., data= train, nvmax = 20)
test.mat <- model.matrix(Y~., data = test)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(train.sub , id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((train$Y - pred)^2)}
#plot MSE for all sizes
plot(val.errors, main = "Training MSE", type = "l")
# (d)
#best subset selection on test set
test.mat <- model.matrix(Y~., data = test)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(train.sub , id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((test$Y - pred)^2)}
#plot MSE for all sizes
plot(val.errors, main = "Test MSE", type = "l")
# (e)
which.min(val.errors) #where the miniumum MSE occurs
#The smallest test MSE occurs at model size 16. We would expect it to not be the largest model, as there should be a penalty for overfitting.
# (f)
coef(train.sub, id = 16) #coefficients at model size 16
#Model performs pretty poorly given that the true zero values where variables 14-19
# (g)
# NOT DONE YET
#(a)
# set seed so random draws always same
set.seed(1)
# define parameters
n <- 1000
p <- 20
# predictor matrix X
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
# coefficients
betas <- rnorm(p)
betas[14:19] = 0
eps <- rnorm(p, mean = 0, sd = 6) #error
Y <- X %*% betas + eps
# Convert to data frame
rand_data <- data.frame(Y = as.vector(Y), X)
#(b)
#split subset randomly
train_index <- sample(1:n, 100,replace=FALSE) #random index for sample
train <- rand_data[train_index, ] #training subset
test <- rand_data[-train_index, ] #test subset
#(c)
#best subset selection on training set
train.sub <- regsubsets(Y ~ ., data= train, nvmax = 20)
train.mat <- model.matrix(Y~., data = train)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(train.sub , id = i)
pred <- train.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((train$Y - pred)^2)}
#plot MSE for all sizes
plot(val.errors, main = "Training MSE", type = "l")
# (d)
#best subset selection on test set
test.mat <- model.matrix(Y~., data = test)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(train.sub , id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((test$Y - pred)^2)}
#plot MSE for all sizes
plot(val.errors, main = "Test MSE", type = "l")
# (e)
which.min(val.errors) #where the miniumum MSE occurs
#The smallest test MSE occurs at model size 16. We would expect it to not be the largest model, as there should be a penalty for overfitting.
# (f)
coef(train.sub, id = 16) #coefficients at model size 16
#Model performs pretty poorly given that the true zero values where variables 14-19
# (g)
# NOT DONE YET
#(a)
# set seed so random draws always same
set.seed(1)
# define parameters
n <- 1000
p <- 20
# predictor matrix X
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
# coefficients
betas <- rnorm(p)
betas[14:19] = 0
eps <- rnorm(p, mean = 0, sd = 1) #error
Y <- X %*% betas + eps
# Convert to data frame
rand_data <- data.frame(Y = as.vector(Y), X)
#(b)
#split subset randomly
train_index <- sample(1:n, 100,replace=FALSE) #random index for sample
train <- rand_data[train_index, ] #training subset
test <- rand_data[-train_index, ] #test subset
#(c)
#best subset selection on training set
train.sub <- regsubsets(Y ~ ., data= train, nvmax = 20)
train.mat <- model.matrix(Y~., data = train)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(train.sub , id = i)
pred <- train.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((train$Y - pred)^2)}
#plot MSE for all sizes
plot(val.errors, main = "Training MSE", type = "l")
# (d)
#best subset selection on test set
test.mat <- model.matrix(Y~., data = test)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(train.sub , id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((test$Y - pred)^2)}
#plot MSE for all sizes
plot(val.errors, main = "Test MSE", type = "l")
# (e)
which.min(val.errors) #where the miniumum MSE occurs
#The smallest test MSE occurs at model size 16. We would expect it to not be the largest model, as there should be a penalty for overfitting.
# (f)
coef(train.sub, id = 16) #coefficients at model size 16
#Model performs pretty poorly given that the true zero values where variables 14-19
# (g)
# NOT DONE YET
#(a)
# set seed so random draws always same
set.seed(1)
# define parameters
n <- 1000
p <- 20
# predictor matrix X
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
# coefficients
betas <- rnorm(p)
betas[10:15] = 0
eps <- rnorm(p, mean = 0, sd = 1) #error
Y <- X %*% betas + eps
# Convert to data frame
rand_data <- data.frame(Y = as.vector(Y), X)
#(b)
#split subset randomly
train_index <- sample(1:n, 100,replace=FALSE) #random index for sample
train <- rand_data[train_index, ] #training subset
test <- rand_data[-train_index, ] #test subset
#(c)
#best subset selection on training set
train.sub <- regsubsets(Y ~ ., data= train, nvmax = 20)
train.mat <- model.matrix(Y~., data = train)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(train.sub , id = i)
pred <- train.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((train$Y - pred)^2)}
#plot MSE for all sizes
plot(val.errors, main = "Training MSE", type = "l")
# (d)
#best subset selection on test set
test.mat <- model.matrix(Y~., data = test)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(train.sub , id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((test$Y - pred)^2)}
#plot MSE for all sizes
plot(val.errors, main = "Test MSE", type = "l")
# (e)
which.min(val.errors) #where the miniumum MSE occurs
#The smallest test MSE occurs at model size 16. We would expect it to not be the largest model, as there should be a penalty for overfitting.
# (f)
coef(train.sub, id = 16) #coefficients at model size 16
#Model performs pretty poorly given that the true zero values where variables 14-19
# (g)
# NOT DONE YET
# (g)
predict.regsubsets <- function(object , newdata , id, ...) {form <- as.formula(object$call [[2]])
+ mat <- model.matrix(form , newdata)
+ coefi <- coef(object , id = id)
+ xvars <- names(coefi)
+ mat[, xvars] %*% coefi
+ }
predict.regsubsets <- function(object , newdata , id, ...) {form <- as.formula(object$call [[2]])
+ mat <- model.matrix(form , newdata)
+ coefi <- coef(object , id = id)
+ xvars <- names(coefi)
+ mat[, xvars] %*% coefi
}
+ mat <- model.matrix(form , newdata)
# (g)
predict.regsubsets <- function(object , newdata , id, ...) {form <- as.formula(object$call [[2]])
+ mat <- model.matrix(form , newdata)
+ coefi <- coef(object , id = id)
+ xvars <- names(coefi)
+ mat[, xvars] %*% coefi}
?coef
coefi
val.errors
View(test.mat)
# (g)
diffs <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(train.sub , id = i)
pred <- test.mat[, names(coefi)] %*% coefi
diffs[i] <- sum((test$Y - pred)^2)}
sqrt.diffs <- sqrt(diffs)
plot(sqrt.diffs)
# The plot of (g) looks very similar to that in part d with the same shape, just different values on the y axis.
which.min(sqrt.diffs)
# (e)
which.min(val.errors) #where the miniumum MSE occurs
rm(list = ls())  # Clear the environment
knitr::opts_knit$set(root.dir = "~/OneDrive - University of North Carolina at Chapel Hill/Spring_25/ECON 573")
library(ISLR2)
library(MASS)
library(e1071)
library(class)
rm(list = ls())  # Clear the environment
knitr::opts_knit$set(root.dir = "~/OneDrive - University of North Carolina at Chapel Hill/Spring_25/ECON 573")
library(ISLR2)
library(MASS)
library(e1071)
library(class)
rmarkdown::render("PS3_P2.Rmd")
rmarkdown::render("~/Library/CloudStorage/OneDrive-UniversityofNorthCarolinaatChapelHill/Spring_25/ECON 573/PS3_P2.Rmd")
update.packages(ask = FALSE, checkBuilt = TRUE)
Yes
tinytex::is_tinytex()
tinytex::tlmgr_update()
